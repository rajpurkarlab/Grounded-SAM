{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model and preprocess function loading for grounded SAM experiment.\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import sys\n",
    "sys.path.extend([\"../\", \"./\"])\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import open_clip\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from segment_anything import sam_model_registry, build_sam, SamPredictor\n",
    "from groundingdino.util.misc import nested_tensor_from_tensor_list\n",
    "\n",
    "from models.grounded_sam import *\n",
    "from models.GroundingDINO.groundingdino.util.inference import load_image\n",
    "from linear_probe import LinearProbe\n",
    "\n",
    "from models.GroundingDINO.groundingdino.models.GroundingDINO.bertwarper import (\n",
    "    generate_masks_with_special_tokens_and_transfer_map_nocate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device=\"cpu\", predictor=False):\n",
    "    \"\"\"Load image encoders, text encoders, and linear probes.\n",
    "    \n",
    "    Load Grounding Dino - model(image encoder, text encoder), linear probe for image embedding and text embedding.\n",
    "         SAM - model(image encoder), linear probe for image embedding\n",
    "         Biomed CLIP - model(image encoder, text encoder), tokenizer for text, preprocess for image\n",
    "    \"\"\"\n",
    "    # Load Grounding Dino\n",
    "    ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "    ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "    ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
    "    groundingdino = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device=device) # groundingdino.backbone, groundingdino.bert, groundingdino.tokenizer\n",
    "    groundingdino.to(device)\n",
    "    \n",
    "    # Load Grounded SAM\n",
    "    sam_checkpoint = './ckpts/sam_vit_l_0b3195.pth'\n",
    "    sam = sam_model_registry[\"vit_l\"](checkpoint=sam_checkpoint)\n",
    "    sam.to(device)\n",
    "    sam_predictor = SamPredictor(sam)\n",
    "\n",
    "    # Load Biomed CLIP\n",
    "    biomedclip, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    biomedclip.to(device)\n",
    "\n",
    "    # Load linear probe for Grounding Dino image embedding\n",
    "    groundingdino_input_dims = [\n",
    "        [1, 256, 28, 28],\n",
    "        [1, 512, 14, 14],\n",
    "        [1, 1024, 7, 7],\n",
    "    ]\n",
    "    groundingdino_img_linear = LinearProbe(\n",
    "        groundingdino_input_dims,\n",
    "        512,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    # Load linear probe for Grounding Dino text embedding\n",
    "    groundingdino_txt_dims = [\n",
    "        [1, 195, 256]\n",
    "    ]\n",
    "    groundingdino_txt_linear = LinearProbe(\n",
    "        groundingdino_txt_dims,\n",
    "        512,\n",
    "        device,\n",
    "    )\n",
    "    \n",
    "    # Load linear probe for SAM image embedding\n",
    "    sam_input_dims = [\n",
    "        [1, 256, 64, 64]\n",
    "    ]\n",
    "    sam_linear = LinearProbe(\n",
    "        sam_input_dims, \n",
    "        512,\n",
    "        device,\n",
    "    )\n",
    "    if predictor:\n",
    "        return groundingdino, sam_predictor, biomedclip, tokenizer, preprocess_train, groundingdino_img_linear, groundingdino_txt_linear, sam_linear\n",
    "\n",
    "    return groundingdino, sam, biomedclip, tokenizer, preprocess_train, groundingdino_img_linear, groundingdino_txt_linear, sam_linear\n",
    "\n",
    "\n",
    "def preprocess_sam(sam, image_path, device=\"cpu\"):\n",
    "    \"\"\"Preprocess image for SAM.\"\"\"\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((20, 20)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    input_image = Image.open(image_path) \n",
    "    input_image_torch = transform(input_image).to(device)\n",
    "\n",
    "    x = input_image_torch\n",
    "    pixel_mean = [123.675, 116.28, 103.53]\n",
    "    pixel_std = [58.395, 57.12, 57.375]\n",
    "    x = (x - torch.Tensor(pixel_mean).view(-1, 1, 1).to(device)) / torch.Tensor(pixel_std).view(-1, 1, 1).to(device)\n",
    "    return x[None, :, :, :]\n",
    "\n",
    "\n",
    "def preprocess_biomedclip(preprocess, tokenizer, image_path, text, device=\"cpu\"):\n",
    "    \"\"\"Preprocess image and text for Biomed CLIP.\"\"\"\n",
    "    bmc_img = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    texts = tokenizer(text, context_length=256).to(device)\n",
    "    return bmc_img, texts\n",
    "\n",
    "\n",
    "def preprocess_groundingdino_img(image_path, device=\"cpu\"):\n",
    "    \"\"\"Preprocess image for Grounding Dino.\"\"\"\n",
    "    _, image = load_image(image_path)\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "    ])\n",
    "    image = transform(image)\n",
    "    image = nested_tensor_from_tensor_list([image]).to(device)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /home/mam0364/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight'])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load model\n",
    "groundingdino, sam, biomedclip, tokenizer, preprocess_train, groundingdino_img_linear, groundingdino_txt_linear, sam_linear = load_model(device)\n",
    "sam = sam\n",
    "biomedclip = biomedclip\n",
    "tokenizer = tokenizer\n",
    "preprocess_train = preprocess_train\n",
    "groundingdino_img_linear = groundingdino_img_linear\n",
    "groundingdino_txt_linear = groundingdino_txt_linear\n",
    "sam_linear = sam_linear\n",
    "\n",
    "# Load preprocess\n",
    "img_path = \"./toy_data/chest_x_ray.jpeg\"\n",
    "text = \"This is a image a 2 lungs.\"\n",
    "\n",
    "sam_img = preprocess_sam(sam, img_path, device)\n",
    "print(\"Sam image embedding shape:\", sam_img.shape)\n",
    "\n",
    "bmc_img, bmc_txt = preprocess_biomedclip(preprocess_train, tokenizer, img_path, text, device)\n",
    "print(\"Biomed CLIP image embedding shape:\", bmc_img.shape)\n",
    "print(\"Biomed CLIP text embedding shape:\", bmc_txt.shape)\n",
    "\n",
    "groundingdino_img = preprocess_groundingdino_img(img_path, device)\n",
    "print(\"Grounding Dino image embedding shape:\", groundingdino_img.shape)\n",
    "\n",
    "        \n",
    "# Generate embedding\n",
    "# SAM image embedding\n",
    "print(\"SAM image shape:\", sam_img.shape)\n",
    "sam_img_embedding = sam.image_encoder(sam_img)\n",
    "print(\"SAM image embedding shape:\", sam_img_embedding.shape)\n",
    "sam_img_embedding = sam_linear(sam_img_embedding)\n",
    "print(\"SAM image embedding shape:\", sam_img_embedding.shape)\n",
    "\n",
    "# Biomed CLIP image + text embeddings\n",
    "bmc_img_embedding = biomedclip.visual(bmc_img)\n",
    "bmc_txt_embedding = biomedclip.encode_text(bmc_txt)\n",
    "print(\"Biomed CLIP image embedding shape:\", bmc_img_embedding.shape)\n",
    "print(\"Biomed CLIP text embedding shape:\", bmc_txt_embedding.shape)\n",
    "\n",
    "# Grounding Dino image embedding\n",
    "backbone_output, _ = groundingdino.backbone(groundingdino_img)\n",
    "groundingdino_img_embedding = []\n",
    "for emb in backbone_output:\n",
    "    groundingdino_img_embedding.append(emb.tensors.to(device))\n",
    "print(\"Grounding Dino image embedding shape:\", groundingdino_img_embedding[0].shape)\n",
    "\n",
    "# Grounding Dino text embedding\n",
    "tokenized = groundingdino.tokenizer(text, padding=\"max_length\", max_length=195, return_tensors=\"pt\")\n",
    "for key, value in tokenized.items():\n",
    "    tokenized[key] = value.to(device)\n",
    "\n",
    "text_self_attention_masks, position_ids = generate_masks_with_special_tokens_and_transfer_map_nocate(\n",
    "    tokenized, groundingdino.specical_tokens, groundingdino.tokenizer\n",
    ")\n",
    "\n",
    "if text_self_attention_masks.shape[1] > groundingdino.max_text_len:\n",
    "    text_self_attention_masks = text_self_attention_masks[\n",
    "        :, : groundingdino.max_text_len, : groundingdino.max_text_len\n",
    "    ]\n",
    "    position_ids = position_ids[:, : groundingdino.max_text_len]\n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][:, : groundingdino.max_text_len]\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][:, : groundingdino.max_text_len]\n",
    "    tokenized[\"token_type_ids\"] = tokenized[\"token_type_ids\"][:, : groundingdino.max_text_len]\n",
    "\n",
    "if groundingdino.sub_sentence_present:\n",
    "    tokenized_for_encoder = {k: v for k, v in tokenized.items() if k != \"attention_mask\"}\n",
    "    tokenized_for_encoder[\"attention_mask\"] = text_self_attention_masks\n",
    "    tokenized_for_encoder[\"position_ids\"] = position_ids\n",
    "else:\n",
    "    tokenized_for_encoder = tokenized\n",
    "\n",
    "bert_output = groundingdino.bert(**tokenized_for_encoder)\n",
    "groundingdino_txt_embedding = groundingdino.feat_map(bert_output[\"last_hidden_state\"]).to(device)\n",
    "print(\"Grounding Dino text embedding shape:\", groundingdino_txt_embedding.shape)\n",
    "\n",
    "\n",
    "groundingdino_img_embedding = groundingdino_img_linear(groundingdino_img_embedding)\n",
    "groundingdino_txt_embedding = groundingdino_txt_linear(groundingdino_txt_embedding)\n",
    "print(\"Grounding Dino image embedding shape:\", groundingdino_img_embedding.shape)\n",
    "print(\"Grounding Dino text embedding shape:\", groundingdino_txt_embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grounded_sam",
   "language": "python",
   "name": "grounded_sam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
